{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒ³ Decision Tree & Random Forest Summary\n",
    "\n",
    "## **1. `rpart` (CART - Classification and Regression Trees)**\n",
    "- Builds a **single** decision tree.\n",
    "- Uses **all data** for training.\n",
    "- Splits are based on **all available features** at each node.\n",
    "- **Prone to overfitting** if the tree grows too deep.\n",
    "- Can be pruned using **`printcp()`** and **`prune()`** to avoid overfitting.\n",
    "\n",
    "### **Useful Functions:**\n",
    "- `rpart()` â†’ Builds a decision tree.\n",
    "- `printcp(tree)` â†’ Displays the complexity parameter (CP) table for pruning.\n",
    "- `prp()` â†’ Simplified tree plot.\n",
    "- `rpart.plot()` â†’ Fully customizable tree visualization.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. `randomForest` (Random Forest)**\n",
    "- Builds **multiple decision trees** (ensemble learning).\n",
    "- Uses **bagging (bootstrap aggregation)**: each tree is trained on a **random subset** of the data.\n",
    "- Uses **random feature selection** at each split to reduce tree correlation.\n",
    "- **Averaging (regression) or majority voting (classification)** improves generalization.\n",
    "\n",
    "### **Key Advantages of Random Forest:**\n",
    "âœ… Reduces **variance** (less overfitting).  \n",
    "âœ… More **robust and accurate** than a single decision tree.  \n",
    "âœ… Works well on large datasets with many features.\n",
    "\n",
    "### **How It Aggregates Predictions:**\n",
    "- **Regression:** Averages the predictions from all trees.\n",
    "- **Classification:** Uses majority voting across trees.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Differences Between `rpart` and `randomForest`**\n",
    "| Feature            | `rpart` (CART)      | `randomForest` |\n",
    "|--------------------|--------------------|---------------|\n",
    "| **Number of Trees** | 1 (Single Tree)    | Multiple Trees |\n",
    "| **Data Used**      | Entire dataset     | Bootstrap sampling |\n",
    "| **Feature Selection** | Uses all features at each split | Random subset at each split |\n",
    "| **Overfitting Risk** | High if not pruned | Low due to averaging |\n",
    "\n",
    "---\n",
    "\n",
    "## **Best Practices**\n",
    "- Use `rpart` when **interpretability** is important and pruning can control overfitting.\n",
    "- Use `randomForest` when **accuracy and generalization** are more important than a single treeâ€™s explainability.\n",
    "- Use `plotcp()` and `prune()` to **optimize an `rpart` tree**.\n",
    "- Use **`randomForest()`** for high-dimensional data with many features.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Code Snippets**\n",
    "\n",
    "#### **Building a Decision Tree with `rpart`**\n",
    "```r\n",
    "library(rpart)\n",
    "tree <- rpart(Kyphosis ~ ., method = \"class\", data = kyphosis)\n",
    "printcp(tree)  # Show complexity parameter table\n",
    "rpart.plot::prp(tree)  # Quick plot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "R"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
